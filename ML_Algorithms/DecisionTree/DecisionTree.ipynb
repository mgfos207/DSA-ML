{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "## Objective\n",
    "\n",
    "The overall objective is to have us split the data into respective nodes based on a criteria we set (entropy, information gain, gini, etc). Once we can no longer split the node because of either max tree depth, minimum samples needed for split or if there is only one type of sample in the node we will return the value of the most_common label in the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas\n",
    "\n",
    "Let's start by setting some formulas that we will use to help us better visualize how this algorithm will work. For this Decision Tree we will use the information gain criteria to greedily splity the nodes from root until the leafe node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "$$E(S) = \\sum \\limits _{i=1} ^{n} p(x^{i}) * log_{2}(p(x^{i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "E(P) = Entropy of Parent\n",
    "\n",
    "E(C) = Entropy of Children\n",
    "\n",
    "$$IG = E(P) - weight+average(E(C))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Information"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
